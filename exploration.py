# -*- coding: utf-8 -*-
"""Exploration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1osXrfDI7Mg9yTUATYZAuQ46Wva2rTQ4T

# 1. Import des bibliothèques et modules utiles
"""

import pandas as pd #Pour la manipulation des DataFrames
import numpy as np #Pour les calcul et conversions de type
import re #Module Regex. Pour chercher, nettoyer ou remplacer du texte dans les tweets
from tqdm import tqdm #Pour afficher une barre de progression lors d’une boucle longue
import matplotlib.pyplot as plt #Pour les graphiques
from transformers import pipeline #Bibliothèque de Hugging Face pour le NLP RoBERTa. Permet d'analyser les sentiments dirrects
import torch #Pour la gestion des tenseurs rapidement

"""# 2. Préparation des données"""

#Fonction de lecture des fichiers
def load_twitter_csv(path):
    return pd.read_csv(
        path,
        sep=";",             #séparateur
        dtype=str,           #tout en string
        encoding="utf-8",
        low_memory=False,
        quotechar='"',       # protège les textes contenant des ;
        on_bad_lines="skip"  # ignore les lignes corrompues
    )

# Chargement des deux DataFrames
trump = load_twitter_csv("data/trump_nort.csv")
biden = load_twitter_csv("data/biden_nort.csv")


#Identification des colonnes de texte. Certains CSV Kaggle ont plusieurs colonnes texte (tweet, text_clean, tweet_original). Cette fonction cherche automatiquement une colonne dont le nom contient "text"
def find_text_column(df):
    for col in df.columns:
        if "text" in col.lower():
            return col
    raise ValueError("Colonne texte introuvable")

trump_text_col = find_text_column(trump)
biden_text_col = find_text_column(biden)

#Vérification
print("Colonne Trump :", trump_text_col)
print("\n")
print("Colonne Biden :", biden_text_col)
print("\n")

"""# 3. Ajout de la colonne candidat au dataset"""

#On ajoute une colonne candidate pour identifier facilement à quel candidat appartient chaque tweet.
trump["candidate"] = "Trump"
biden["candidate"] = "Biden"

#Concatenation des deux DataFrames
df = pd.concat([trump, biden], ignore_index=True)
print("Dataset combiné :", df.shape)

#Echantillonnage car l'analyse sur tout le jeu de donnée prend trop de temps
MAX_TWEETS = 10  # par candidat

df_sample = (
    df.groupby("candidate", group_keys=False)
      .apply(lambda x: x.sample(n=min(MAX_TWEETS, len(x)), random_state=42))
)

print("Dataset échantillonné :", df_sample.shape)

trump_sample = df_sample[df_sample['candidate'] == 'Trump']
biden_sample = df_sample[df_sample['candidate'] == 'Biden']

"""#  4. Préparation du modèle Hugging Face"""

#Chargement du modèle avec l'interface simplifiée de HF.
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model="cardiffnlp/twitter-xlm-roberta-base-sentiment", #Modèle pré-entrainé
    tokenizer="cardiffnlp/twitter-xlm-roberta-base-sentiment", #Transforme le texte en token que le modèle peut comprendre
    device=0  #Pour utiliser le GPU. Changer par -1 si vous vous utiliser le CPU
)

"""# 5. Analyse des sentiments"""

torch.set_grad_enabled(False) #Désactivation du gradient

def analyze_sentiment(texts, batch_size=64): #Fonction d'analyse de tweets par batch
    results = []
    texts = texts.tolist()

    for i in tqdm(range(0, len(texts), batch_size)):
        batch = texts[i:i + batch_size]
        preds = sentiment_pipeline(batch)
        results.extend(preds)

    return results

print("Analyse des sentiments...")

#Implémentation sur les échantillons
trump_sentiments = analyze_sentiment(trump_sample[trump_text_col]) #Label du tweet et score du tweet entre 0 et 1
biden_sentiments = analyze_sentiment(biden_sample[biden_text_col])

#Ajout des résultats dans le DataFrame
trump_sample.loc[:, "sentiment"] = [s["label"] for s in trump_sentiments]
trump_sample.loc[:, "score"]     = [s["score"] for s in trump_sentiments]

biden_sample.loc[:, "sentiment"] = [s["label"] for s in biden_sentiments]
biden_sample.loc[:, "score"]     = [s["score"] for s in biden_sentiments]

"""# 6. Résumé statistiques"""

summary_trump = trump_sample.groupby("sentiment").size().reset_index(name="count")
summary_biden = biden_sample.groupby("sentiment").size().reset_index(name="count")

summary_trump["candidate"] = "Trump"
summary_biden["candidate"] = "Biden"

# Combiner les deux
summary = pd.concat([summary_trump, summary_biden], ignore_index=True)
print(summary)

"""# 7. Sauvegarde dans un fichier CSV"""

# On combine les deux échantillons (Trump + Biden)
df_sample = pd.concat([trump_sample, biden_sample], ignore_index=True)
print(df_sample.head())
df_sample.to_csv("tweets_with_sentiment.csv", index=False)
summary.to_csv("sentiment_summary.csv", index=False)

print("\n Analyse terminée et sauvegardée")

"""# 8. Visualisation"""

import seaborn as sns
import matplotlib.pyplot as plt
# Concaténer les DataFrames analysés
df_result = pd.concat([trump_sample, biden_sample], ignore_index=True)

# Vérifier que tout est correct
print(df_result[["candidate", trump_text_col, "sentiment", "score"]])

plt.figure(figsize=(6,4))
sns.countplot(data=df_result, x="sentiment", hue="candidate")
plt.title("Répartition des sentiments par candidat")
plt.ylabel("Nombre de tweets")
plt.show()

for candidate in ["Trump", "Biden"]:
    data = df_result[df_result["candidate"] == candidate]["sentiment"].value_counts()
    plt.figure(figsize=(5,5))
    plt.pie(data, labels=data.index, autopct="%1.1f%%", colors=["red","grey","green"])
    plt.title(f"Répartition des sentiments - {candidate}")
    plt.show()